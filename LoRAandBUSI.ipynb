{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1ZosYoE6gem_dXL89WVzaFtf_YWY3AVwo",
      "authorship_tag": "ABX9TyOs388M+GHSI1lUkePBOTeZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditiprashant07/BUSI-and-LoRA-Project-/blob/main/LoRAandBUSI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJGUWZLfO15k"
      },
      "outputs": [],
      "source": [
        "\"\"\"Google Collabs runtime is temporary and to ensure it persists after disconnection. Without this you lose everything.\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"This is the best practice to organize data\"\"\"\n",
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/BUSI_LoRA_Project\"\n",
        "\n",
        "folders = [\"data\", \"checkpoints\", \"outputs\"]\n",
        "\n",
        "for folder in folders:\n",
        "    os.makedirs(os.path.join(PROJECT_ROOT, folder), exist_ok=True)\n",
        "\n",
        "print(\"Project folders created successfully!\")\n"
      ],
      "metadata": {
        "id": "UZbmz1itQ-Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"To check if torch and the specific GPU type is available\"\"\"\n",
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "id": "1VBP80y2RbKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Transformers -> Hugging Face pre-trained library\n",
        "peft -> Parameter-Efficient Fine Tuning which enables LoRA\n",
        "scikit-learn -> Metrics, Confusion Matrix, Classification report\n",
        "Matplotlib -> For plotting training curves\n",
        "GRAD-CAM -> Explainability - showing what the model sees\"\"\"\n",
        "!pip install transformers\n",
        "!pip install -q peft\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q matplotlib"
      ],
      "metadata": {
        "id": "6RdBVg2YRMjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing Kaggle from where our dataset will be pulled\n",
        "!pip install -q kaggle\n"
      ],
      "metadata": {
        "id": "eI3HujM0S2ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is important to create a kaggle.json file and for connecting to your Kaggle account.\n",
        "import json\n",
        "import os\n",
        "\n",
        "kaggle_username = \"username\"\n",
        "kaggle_key = \"token\"\n",
        "\n",
        "kaggle_json = {\n",
        "    \"username\": kaggle_username,\n",
        "    \"key\": kaggle_key\n",
        "}\n",
        "\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w\") as f:\n",
        "    json.dump(kaggle_json, f)\n",
        "\n",
        "os.chmod(\"/root/.kaggle/kaggle.json\", 600)\n",
        "\n",
        "print(\"kaggle.json created successfully!\")\n"
      ],
      "metadata": {
        "id": "o1jFwlE1S1mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets list -s breast-ultrasound\n"
      ],
      "metadata": {
        "id": "qGh17fGqS6fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d aryashah2k/breast-ultrasound-images-dataset\n"
      ],
      "metadata": {
        "id": "F3za-9C9S-el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip breast-ultrasound-images-dataset.zip -d data/\n"
      ],
      "metadata": {
        "id": "-0qcwBvtTALF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"In each transformation of image we have a resize since ViT an DeiT require a 224x224 ImageNet Images.\n",
        "After resizing the Image is converted into a tensor value between 0 and 1.\n",
        "The Normalization is used at the end since ViT was trained on these standard values. Without this the model sees data which is out of it's distribution\"\"\"\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# ImageNet normalization values (required for pretrained ViT)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n"
      ],
      "metadata": {
        "id": "I3L5HQUZUic0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"ImageFolder automatically assigns ;abels based on folder names. This is to ensure data remains consistent\"\"\"\n",
        "dataset_path = \"data/Dataset_BUSI_with_GT\"\n",
        "\n",
        "full_dataset = datasets.ImageFolder(\n",
        "    root=dataset_path,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "print(\"Total images:\", len(full_dataset))\n",
        "print(\"Classes:\", full_dataset.classes)\n"
      ],
      "metadata": {
        "id": "CRNrYC7vUrwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"This is standard pattern for small datasets and a random split ensures random distributions\"\"\"\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "print(\"Train size:\", len(train_dataset))\n",
        "print(\"Validation size:\", len(val_dataset))\n"
      ],
      "metadata": {
        "id": "vXwU9ttSUy1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Data Loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n"
      ],
      "metadata": {
        "id": "kJosFEX4U6Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing One Batch to check if pre processing works\n",
        "images, labels = next(iter(train_loader))\n",
        "print(\"Image batch shape:\", images.shape)\n",
        "print(\"Label batch shape:\", labels.shape)\n"
      ],
      "metadata": {
        "id": "7wNPh7D-U-hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading our vision transformer-facebook/deit-small-patch16-224\n",
        "import torch\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_name = \"facebook/deit-small-patch16-224\"\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3,\n",
        "    ignore_mismatched_sizes=True # Add this line to handle the mismatch\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "print(\"Model loaded successfully.\")"
      ],
      "metadata": {
        "id": "0zMkoYC_VXwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a sanity check\n",
        "outputs = model(images.to(device))\n",
        "print(outputs.logits.shape)\n"
      ],
      "metadata": {
        "id": "_k68Gat7VkXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we begin training the LoRA layers and the Classification Heads- We also need to freeze the base model in this step\n",
        "for param in model.base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(\"Base model frozen.\")\n"
      ],
      "metadata": {
        "id": "ZBTx9ip5V6mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We apply LoRA to the attention layers\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\"\n",
        "    # Removed: task_type=\"IMAGE_CLASSIFICATION\" as it's not a valid type\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "c1SJrfTwWCfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Optimizer and Loss\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=5e-5\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "mnVdgt-kWoxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = torch.cuda.amp.GradScaler()\n"
      ],
      "metadata": {
        "id": "acnAndCAXCM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a clean and simple training loop\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "F20fBqj9Wr8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this for validation accuracy\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\"Validation Accuracy:\", 100 * correct / total, \"%\")\n"
      ],
      "metadata": {
        "id": "hTORO4WKWyOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "print(classification_report(all_labels, all_preds, target_names=full_dataset.classes))\n"
      ],
      "metadata": {
        "id": "Oae5OdUhX5a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Grad-CAM for medical imaging explainability\n",
        "!pip install grad-cam -q\n"
      ],
      "metadata": {
        "id": "O-Bq28VFYpAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n"
      ],
      "metadata": {
        "id": "zKcN0x_aYxta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Put model in EVAL mode\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "bY3610dvakLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Reshape Function\n",
        "def reshape_transform(tensor):\n",
        "    # Remove class token\n",
        "    tensor = tensor[:, 1:, :]\n",
        "\n",
        "    h = w = 14  # because image is 224 and patch size is 16 (224/16=14)\n",
        "\n",
        "    tensor = tensor.reshape(tensor.size(0), h, w, tensor.size(2))\n",
        "    tensor = tensor.permute(0, 3, 1, 2)  # (B, C, H, W)\n",
        "\n",
        "    return tensor\n"
      ],
      "metadata": {
        "id": "rfZPV8V_amco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap the model\n",
        "class ModelWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x).logits\n",
        "\n",
        "wrapped_model = ModelWrapper(model)\n",
        "wrapped_model.eval()\n"
      ],
      "metadata": {
        "id": "H3LUb_jZaqRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select correct target layer\n",
        "target_layer = model.base_model.model.vit.encoder.layer[-1].layernorm_after\n"
      ],
      "metadata": {
        "id": "8K-TNqnIat7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize GRAD-CAM\n",
        "cam = GradCAM(\n",
        "    model=wrapped_model,\n",
        "    target_layers=[target_layer],\n",
        "    reshape_transform=reshape_transform\n",
        ")\n"
      ],
      "metadata": {
        "id": "N9mDHwZ6ax9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one validation image\n",
        "images, labels = next(iter(val_loader))\n",
        "\n",
        "image = images[0].unsqueeze(0).to(device)\n",
        "label = labels[0].item()\n"
      ],
      "metadata": {
        "id": "L-t7qNHqa3zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate GRAD-CAM heatmap\n",
        "targets = [ClassifierOutputTarget(label)]\n",
        "\n",
        "grayscale_cam = cam(input_tensor=image, targets=targets)\n",
        "grayscale_cam = grayscale_cam[0]\n"
      ],
      "metadata": {
        "id": "QJ8LvQspa62w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Undo normalization\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "img = images[0].permute(1, 2, 0).cpu().numpy()\n",
        "img = std * img + mean\n",
        "img = np.clip(img, 0, 1)\n"
      ],
      "metadata": {
        "id": "N87e3NEEa-P3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overlap Heatmap\n",
        "visualization = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(visualization)\n",
        "plt.title(f\"True Label: {full_dataset.classes[label]}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pbhpCy6NbBSq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}